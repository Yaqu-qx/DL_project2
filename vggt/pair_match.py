import torch
import numpy as np
import cv2
import matplotlib.pyplot as plt
from vggt.models.vggt import VGGT
from vggt.utils.load_fn import load_and_preprocess_images
import os
import random

def extract_vggt_features(model, images, device):
    """
    ä½¿ç”¨VGGTæå–å›¾åƒç‰¹å¾
    """
    with torch.no_grad():
        images_batch = images.unsqueeze(0)
        aggregated_tokens_list, ps_idx = model.aggregator(images_batch)
        point_map, point_conf = model.point_head(aggregated_tokens_list, images_batch, ps_idx)
        return point_map, point_conf

def match_features(desc1, desc2, max_matches=500):
    """
    ä½¿ç”¨BFMatcherè¿›è¡Œç‰¹å¾åŒ¹é…
    """
    print("?")
    desc1_np = desc1.cpu().numpy().reshape(-1, desc1.shape[-1]).astype(np.float32)
    desc2_np = desc2.cpu().numpy().reshape(-1, desc2.shape[-1]).astype(np.float32)
    print("??")
    # L2å½’ä¸€åŒ–
    desc1_np /= np.linalg.norm(desc1_np, axis=1, keepdims=True) + 1e-6
    desc2_np /= np.linalg.norm(desc2_np, axis=1, keepdims=True) + 1e-6
    print(desc1_np , desc2_np )

    if desc1_np.shape[0] == 0 or desc2_np.shape[0] == 0:
        print("âš ï¸ è½¬æ¢åç‰¹å¾ä¸ºç©ºï¼Œè·³è¿‡åŒ¹é…")
        return [], desc1_np, desc2_np

    bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)
    print("????")
    matches = bf.match(desc1_np, desc2_np)
    matches = sorted(matches, key=lambda x: x.distance)

    num_to_keep = min(max_matches, len(matches))
    good_matches = matches[:num_to_keep]

    return good_matches, desc1_np, desc2_np

def generate_keypoints(desc, H, W):
    """
    å°†ç‰¹å¾æè¿°ç¬¦æ˜ å°„ä¸ºå›¾åƒå…³é”®ç‚¹åæ ‡
    """
    keypoints = []
    for idx in range(H * W):
        y = idx // W
        x = idx % W
        keypoints.append(cv2.KeyPoint(float(x), float(y), 2.0))  # size=2.0ï¼Œçº¿ä¼šæ›´ç²—
    return keypoints

def visualize_matches(image1_path, image2_path, matches, keypoints1, keypoints2, output_path="matches_visualization.png", max_lines=30):
    """
    å¯è§†åŒ–åŒ¹é…ç‚¹
    """
    img1 = cv2.imread(image1_path)
    img2 = cv2.imread(image2_path)
    
    # éšæœºå‡åŒ€é‡‡æ ·åŒ¹é…çº¿
    if len(matches) > max_lines:
        matches_to_draw = random.sample(matches, max_lines)
    else:
        matches_to_draw = matches

    match_img = cv2.drawMatches(
        img1, keypoints1, img2, keypoints2,
        matches_to_draw, None,
        matchColor=(0,255,0),
        singlePointColor=(255,0,0),
        flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS
    )

    plt.figure(figsize=(15, 10))
    plt.imshow(cv2.cvtColor(match_img, cv2.COLOR_BGR2RGB))
    plt.axis('off')
    plt.title(f"VGGT Feature Matches ({len(matches_to_draw)} matches)")
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    plt.close()
    print(f"âœ… åŒ¹é…å¯è§†åŒ–å·²ä¿å­˜: {output_path}")

def main():
    # device = "cuda" if torch.cuda.is_available() else "cpu"
    device = "cpu"
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")

    # åˆå§‹åŒ–æ¨¡å‹
    model = VGGT()
    local_model_path = "./model.pt"
    if os.path.exists(local_model_path):
        state_dict = torch.load(local_model_path, map_location=device)
        model.load_state_dict(state_dict)
        model.to(device)
    else:
        print("æ— æ³•åŠ è½½æ¨¡å‹ï¼Œè¯·ç¡®ä¿æ¨¡å‹æ–‡ä»¶å­˜åœ¨")
        return
    model.eval()

    # å›¾åƒè·¯å¾„
    image_paths = [
        "../dataset-advance/bdaibdai___MatrixCity/aerial_street_fusion/aerial/0060.png",
        "../dataset-advance/bdaibdai___MatrixCity/aerial_street_fusion/street/test/0042.png",
    ]

    # åŠ è½½å’Œé¢„å¤„ç†å›¾åƒ
    images = load_and_preprocess_images(image_paths).to(device)

    # æå–ç‰¹å¾
    print("ğŸ” æ­£åœ¨æå–VGGTç‰¹å¾...")
    point_map, point_conf = extract_vggt_features(model, images, device)

    # å‡è®¾è¾“å‡ºç‰¹å¾å›¾ä¸º [B, num_images, H, W, C]
    H, W = point_map.shape[2], point_map.shape[3]
    desc1 = point_map[0, 0]  # ç¬¬ä¸€å¼ å›¾
    desc2 = point_map[0, 1]  # ç¬¬äºŒå¼ å›¾

    print(f"âœ… æ¯å¼ å›¾ä½¿ç”¨ {H*W} ä¸ªç‰¹å¾ç‚¹è¿›è¡ŒåŒ¹é…")
    matches, desc1_np, desc2_np = match_features(desc1, desc2, max_matches=500)
    print(matches, desc1_np, desc2_np)
    
    if len(matches) < 20:
        print(f"âš ï¸ åŒ¹é…æ•°å¤ªå°‘: {len(matches)}")
        return

    keypoints1 = generate_keypoints(desc1, H, W)
    keypoints2 = generate_keypoints(desc2, H, W)

    print("ğŸ”— æ­£åœ¨è¿›è¡Œç‰¹å¾åŒ¹é…...")
    visualize_matches(image_paths[0], image_paths[1], matches, keypoints1, keypoints2, output_path="../advance-output/vggt-3/60-42.png", max_lines=30)

    # æ‰“å°åŒ¹é…è·ç¦»ç»Ÿè®¡
    distances = [m.distance for m in matches]
    print(f"ğŸ“Š åŒ¹é…è·ç¦»ç»Ÿè®¡: æœ€å°={min(distances):.4f}, æœ€å¤§={max(distances):.4f}, å¹³å‡={np.mean(distances):.4f}")
    print(f"âœ… æˆåŠŸæå–å¹¶åŒ¹é… {len(matches)} å¯¹ç‰¹å¾ç‚¹")

if __name__ == "__main__":
    main()
